#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score
import lightgbm

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_train = 'training'
channel_pretrain_model = 'pretrain_model'
train_path = os.path.join(input_path, channel_train)
pretrain_model_path = os.path.join(input_path, channel_pretrain_model)

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # get hyperparameters
        training_params = get_training_params()
        print(training_params)
        # get data
        X, y = get_data(train_path, channel_train)
        X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)
        # traiing model
        lgb_train = lightgbm.Dataset(X_train, y_train, free_raw_data=False)
        lgb_eval = lightgbm.Dataset(X_eval, y_eval, free_raw_data=False)
        num_boost_round = training_params['num_boost_round']
        early_stopping_rounds = training_params['early_stopping_rounds']
        if training_params['user_model_path']:
            model_file_path = get_pretrain_model(pretrain_model_path) 
            model = lightgbm.train(
                            training_params,
                            lgb_train,
                            init_model=model_file_path,
                            valid_sets=[lgb_eval, lgb_train],
                            valid_names=['eval', 'train'],
                            num_boost_round=num_boost_round,
                            early_stopping_rounds=early_stopping_rounds,
                            verbose_eval=True
                            )
        else:
            model = lightgbm.train(
                            training_params,
                            lgb_train,
                            valid_sets=[lgb_eval, lgb_train],
                            valid_names=['eval', 'train'],
                            num_boost_round=num_boost_round,
                            early_stopping_rounds=early_stopping_rounds,
                            verbose_eval=True
                            )

        # output model scores
        log_model_score(model, X_train, y_train, 'train')
        log_model_score(model, X_eval, y_eval, 'eval')
        # save the model
        save_model(model)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def get_training_params():
    # Read in any hyperparameters that the user passed with the training job
    with open(param_path, 'r') as tc:
        training_params = json.load(tc)

    param_settings = [
        ['boosting_type', 'gbdt', str],
        ['class_weight', None, str],
        ['colsample_bytree', 1.0, float],
        ['learning_rate', 0.1, float],
        ['max_depth', -1, int],
        ['min_child_samples', 20, int],
        ['min_child_weight', 0.001, float],
        ['min_split_gain', 0.0, float],
        ['n_estimators', 100, int],
        ['num_leaves', 31, int],
        ['random_state', None, int],
        ['reg_alpha', 0.0, float],
        ['reg_lambda', 0.0, float],
        ['subsample', 1.0, float],
        ['objective', 'binary', str],
        ['metric', 'auc', str],
        ['user_model_path', False, bool],
        ['num_boost_round', 100, int],
        ['early_stopping_rounds', 10, int],
    ]

    for key, default, value_type in param_settings:
        if default is None:
            training_params[key] = training_params.get(key, default)
            if training_params[key] is not None:
                training_params[key] = value_type(training_params[key])
        else:
            training_params[key] = value_type(training_params.get(key, default))

    return training_params

def get_data(path, channel_name):
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(path, file) for file in os.listdir(path) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                            'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                            'the data specification in S3 was incorrectly specified or the role specified\n' +
                            'does not have permission to access the data.').format(path, channel_name))
    raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
    data = pd.concat(raw_data)
    # labels are in the first column
    y = data.ix[:,0]
    X = data.ix[:,1:]
    return X, y

def get_pretrain_model(path):
    model_file_path = os.path.join(path, os.listdir(path)[0])
    return model_file_path

def save_model(model):
    model.save_model(os.path.join(model_path,'model.txt'))

def log_model_score(model, X, y, score_type='train'):
    predict_proba = model.predict(X)
    prediction = np.round(predict_proba)

    accuracy = accuracy_score(y, prediction)
    precision = precision_score(y, prediction)
    recall = recall_score(y, prediction)
    auc = roc_auc_score(y, predict_proba)
    f1 = f1_score(y, prediction)

    print(score_type + "_accuracy={:.4f};".format(accuracy))
    print(score_type + "_precision={:.4f};".format(precision))
    print(score_type + "_recall={:.4f};".format(recall))
    print(score_type + "_auc={:.4f};".format(auc))
    print(score_type + "_f1={:.4f};".format(f1))

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
