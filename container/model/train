#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score
import lightgbm

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_train = 'train'
channel_test = 'test'
train_path = os.path.join(input_path, channel_train)
test_path = os.path.join(input_path, channel_test)

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # get hyperparameters
        training_params = get_training_params()
        print(training_params)
        # get data
        X_train, y_train = get_data(train_path, channel_train)
        X_eval, y_eval = get_data(test_path, channel_test)
        # traiing model
        lgb_train = lightgbm.Dataset(X_train, y_train, free_raw_data=False)
        lgb_eval = lightgbm.Dataset(X_eval, y_eval, free_raw_data=False)
        num_boost_round = training_params['num_boost_round']
        early_stopping_rounds = training_params['early_stopping_rounds']    
        if training_params['user_model_path'] is None:
            model = lightgbm.train(
                            training_params,
                            lgb_train,
                            valid_sets=[lgb_eval, lgb_train],
                            valid_names=['eval', 'train'],
                            num_boost_round=num_boost_round,
                            early_stopping_rounds=early_stopping_rounds,
                            verbose_eval=True
                            )
        else:
            model = lightgbm.train(
                            training_params,
                            lgb_train,
                            init_model=training_params['user_model_path'],
                            valid_sets=[lgb_eval, lgb_train],
                            valid_names=['eval', 'train'],
                            num_boost_round=num_boost_round,
                            early_stopping_rounds=early_stopping_rounds,
                            verbose_eval=True
                            )

        # output model scores
        log_model_score(model, X_train, y_train, 'train')
        log_model_score(model, X_eval, y_eval, 'eval')
        # save the model
        save_model(model)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def get_training_params():
    # Read in any hyperparameters that the user passed with the training job
    with open(param_path, 'r') as tc:
        training_params = json.load(tc)

    boosting_type = training_params.get('boosting_type', 'gbdt')
    class_weight =  training_params.get('class_weight', None)
    colsample_bytree = float(training_params.get('colsample_bytree', 1.0))
    learning_rate = float(training_params.get('learning_rate', 0.1))
    max_depth = int(training_params.get('max_depth', -1))
    min_child_samples = int(training_params.get('min_child_samples', 20))
    min_child_weight = float(training_params.get('min_child_weight', 0.001))
    min_split_gain = float(training_params.get('min_split_gain', 0.0))
    n_estimators = int(training_params.get('n_estimators', 100))
    num_leaves = int(training_params.get('num_leaves', 31))
    objective =  training_params.get('objective', None)
    random_state = training_params.get('random_state', None)
    if random_state is not None:
        random_state = int(random_state)
    reg_alpha = float(training_params.get('reg_alpha', 0.0))
    reg_lambda = float(training_params.get('reg_lambda', 0.0))
    subsample = float(training_params.get('subsample', 1.0))
    user_model_path = training_params.get('user_model_path', None)
    num_boost_round = int(training_params.get('num_boost_round', 100))
    early_stopping_rounds = int(training_params.get('early_stopping_rounds', 10))

    training_params['boosting_type'] = boosting_type
    training_params['class_weight'] = class_weight
    training_params['colsample_bytree'] = colsample_bytree
    training_params['learning_rate'] = learning_rate
    training_params['max_depth'] = max_depth
    training_params['min_child_samples'] = min_child_samples
    training_params['min_child_weight'] = min_child_weight
    training_params['min_split_gain'] = min_split_gain
    training_params['n_estimators'] = n_estimators
    training_params['num_leaves'] = num_leaves
    training_params['objective'] = objective
    training_params['random_state'] = random_state
    training_params['reg_alpha'] = reg_alpha
    training_params['reg_lambda'] = reg_lambda
    training_params['subsample'] = subsample
    training_params['user_model_path'] = user_model_path
    training_params['num_boost_round'] = num_boost_round
    training_params['early_stopping_rounds'] = early_stopping_rounds
    training_params['objective'] = 'binary'
    training_params['metric'] = 'auc'

    return training_params

def get_data(path, channel_name):
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(path, file) for file in os.listdir(path) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                            'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                            'the data specification in S3 was incorrectly specified or the role specified\n' +
                            'does not have permission to access the data.').format(path, channel_name))
    raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
    data = pd.concat(raw_data)
    # labels are in the first column
    y = data.ix[:,0]
    X = data.ix[:,1:]
    return X, y

def save_model(model):
    model.save_model(os.path.join(model_path,'model.txt'))

def log_model_score(model, X, y, score_type='train'):
    predict_proba = model.predict(X)
    prediction = np.round(predict_proba)

    accuracy = accuracy_score(y, prediction)
    precision = precision_score(y, prediction)
    recall = recall_score(y, prediction)
    auc = roc_auc_score(y, predict_proba)
    f1 = f1_score(y, prediction)

    print(score_type + "_accuracy={:.4f};".format(accuracy))
    print(score_type + "_precision={:.4f};".format(precision))
    print(score_type + "_recall={:.4f};".format(recall))
    print(score_type + "_auc={:.4f};".format(auc))
    print(score_type + "_f1={:.4f};".format(f1))

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
