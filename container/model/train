#!/usr/bin/env python

# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score
from keras.models import Sequential,Model
from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Dropout, Flatten, concatenate, BatchNormalization, Lambda
import keras.backend as K
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import tensorflow as tf

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # get hyperparameters
        training_params = get_training_params()
        # get training data
        X, y = get_training_data()
        # traiing model
        fit(X, y, training_params)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def get_training_params():
    # Read in any hyperparameters that the user passed with the training job
    with open(param_path, 'r') as tc:
        training_params = json.load(tc)
    EPOCHS = training_params.get('EPOCHS', 100)
    EPOCHS = int(EPOCHS)

    BATCH_SIZE = training_params.get('BATCH_SIZE', 64)
    BATCH_SIZE = int(BATCH_SIZE)

    training_params['EPOCHS'] = EPOCHS
    training_params['BATCH_SIZE'] = BATCH_SIZE
    
    return training_params

def get_training_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                            'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                            'the data specification in S3 was incorrectly specified or the role specified\n' +
                            'does not have permission to access the data.').format(training_path, channel_name))
    raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
    train_data = pd.concat(raw_data)
    # labels are in the first column
    y = train_data.ix[:,0]
    X = train_data.ix[:,1:]
    return X, y

def fit(X, y, training_params):
    model = get_model(X.shape)

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)
    X_train = X_train.values.reshape(-1, X_train.shape[1], 1)
    X_val = X_val.values.reshape(-1, X_val.shape[1], 1)

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['acc',roc_auc]
    )
    
    model_check_point = ModelCheckpoint('best.hdf5', save_best_only=True)
    early_stopping = EarlyStopping(patience=10)
    reduce_lr = ReduceLROnPlateau(patience=3)
    callbacks = [model_check_point,early_stopping,reduce_lr]

    history = model.fit(
        X_train, y_train,
        batch_size=training_params['BATCH_SIZE'],
        epochs=training_params['EPOCHS'],
        validation_data=(X_val, y_val),
        callbacks=callbacks)

    model.load_weights('best.hdf5')
    model.save(os.path.join(model_path, 'model.h5'))

def roc_auc(y_true, y_pred):
    roc_auc = tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)
    return roc_auc

def get_model(input_shpae):
    inp = Input(batch_shape=(None, input_shpae[1], 1))
    x = Conv1D(128, 4, padding='same')(inp)
    x = BatchNormalization()(x)
    pool1 = MaxPooling1D(2, padding='same')(x)
    x = Conv1D(64, 4, padding='same')(pool1)
    x = BatchNormalization()(x)
    pool2 = MaxPooling1D(2, padding='same')(x)
    x = Conv1D(32, 4, padding='same')(pool2)
    x = Dense(256)(x)
    x = Dropout(0.3)(x)
    x = Dense(128)(x)
    x = Dropout(0.3)(x)
    x = Dense(64)(x)
    x = Dropout(0.3)(x)
    x = Dense(32)(x)
    x = Flatten()(x)
    x = Dense(1, activation='sigmoid')(x)
    return Model(inputs=inp, outputs=x)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
